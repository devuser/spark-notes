<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Spark-notes by devuser</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Spark-notes</h1>
        <h2>Note anything during writing spark or scala hadoop golfing gopher python review 大数据 面试题 面试 BAT</h2>

        <section id="downloads">
          <a href="https://github.com/devuser/spark-notes/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/devuser/spark-notes/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/devuser/spark-notes" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h1>
<a id="sparknotes" class="anchor" href="#sparknotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>sparknotes</h1>

<p>Note anything during writing spark or scala python php golang gopher bigdata hadoop review 大数据 面试 面试题 大数据面试题</p>

<p><a href="stepbystepfromsrc.md">从源代码开始</a></p>

<p><a href="powerdesigner.md">PowerDesigner</a></p>

<p><a href="spark-bat.md">SPARK解决BAT问题</a></p>

<p><a href="case1_wordcount.md">编写第一个Spark程序</a></p>

<p><a href="MLlib.md">机器学习</a></p>

<p><a href="recommenddation_in_douban.md">推荐在豆瓣</a></p>

<p><a href="bigdatareview.md">大数据面试题</a></p>

<p>本系列文档基于Spark 1.5撰写。</p>

<h2>
<a id="幸福生活从docker开始" class="anchor" href="#%E5%B9%B8%E7%A6%8F%E7%94%9F%E6%B4%BB%E4%BB%8Edocker%E5%BC%80%E5%A7%8B" aria-hidden="true"><span class="octicon octicon-link"></span></a>幸福生活从Docker开始</h2>

<p>在DockerHub搜索Spark <code>docker search -s 2 spark</code></p>

<p>存在若干个Spark镜像，推荐选择 _ sequenceiq/spark _ 执行如下命令拉取 ｀docker pull sequenceiq/spark`</p>

<p>有强迫症的同学建议执行如下命令拉取该镜像的所有版本</p>

<p><code>docker pull -a sequenceiq/spark</code></p>

<p>拉取所有的版本是有意义的，因为Spark处在快速成长期，年初到现在从_ 1.2 _ 版本迭代到_ 1.5 _</p>

<p>目前市面上流行的印刷品的书，多数还是基于_ 1.2 _ 版本。 作为初学者来说，遇到版本差异，会非常痛苦。</p>

<p>所以我建议，使用与您印刷品的书一致的Spark版本。</p>

<h1>
<a id="机器学习框架spark-mllib" class="anchor" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6spark-mllib" aria-hidden="true"><span class="octicon octicon-link"></span></a><strong>机器学习框架（Spark MLlib)</strong>
</h1>

<p>目前支持4种常见的机器学习问题：二元分类、回归、聚类以及协同过滤 - 依赖 （将会调用jblas线性代数库，这个库本身依赖于原生的Fortran程序 如果想用Python调用MLlib，需要安装NumPy 1.7或更新的版本 - 二元分类</p>

<pre><code>是个监督学习问题。 目前支持两个适用于二元分类的标准模型家族：线性支持向量机（SVMs）和逻辑回归， 同时也分别适用于这两个模型家族的L1和L2正则化变体。

这些训练算法都利用了一个底层的梯度下降基础算法。 二元分类算法的输入值是一个正则项参数（regParam）和多个与梯度下降相关的参数（stepSize，numIterations，miniBatchFraction）

目前可用的二元分类算法：
- **SVMWithSGD**
- **LogisticRegressWithSGD**
</code></pre>

<ul>
<li>线性回归</li>
<li>聚类</li>
<li>协同过滤 隐性反馈与协同反馈</li>
<li>梯度下降基础算法</li>
<li>二元分类</li>
<li>线性回归</li>
<li>聚类</li>
<li>协同过滤</li>
</ul>

<h1>
<a id="搭建hadoop单机版本和伪分布式开发环境" class="anchor" href="#%E6%90%AD%E5%BB%BAhadoop%E5%8D%95%E6%9C%BA%E7%89%88%E6%9C%AC%E5%92%8C%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83" aria-hidden="true"><span class="octicon octicon-link"></span></a>搭建Hadoop单机版本和伪分布式开发环境</h1>

<p><code>sudo -s</code>进入<strong>root</strong>用户权限模式</p>

<p><code>apt-get install vim</code></p>

<p>Hadoop是采用SSH进行通讯的，此时要设置密码为空， 即不需要密码登录，这样免去每次通讯时都输入密码。</p>

<p><code>apt-get install ssh</code></p>

<p>安装完毕后启动SSH服务</p>

<p><code>/etc/init.d/ssh start</code></p>

<p>以下命令验证服务是否正常启动</p>

<p><code>ps -e|grep ssh</code></p>

<p>如下命令产生私钥和公钥</p>

<p><code>ssh-keygen -t rsa -P ""</code></p>

<p>使用Java自带的jps命令查询出所有的守护进程:</p>

<p>** <a href="https://github.com/todo" class="user-mention">@todo</a>插入图片** - 创建HDFS的文件夹<em>/input</em> <code>bin/hadoop dfs -mkdir /input</code> - 复制本地的配置文件到HDFS文件夹<em>/input* `bin/hadoop dfs -copyFromLocal etc/hadoop/</em>.xml /input<code>- 在刚刚构建的伪分布式模式下运行自带的_wordcount_程序</code>bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /input /output`</p>

<h1>
<a id="开始第一个测试案例" class="anchor" href="#%E5%BC%80%E5%A7%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%B5%8B%E8%AF%95%E6%A1%88%E4%BE%8B" aria-hidden="true"><span class="octicon octicon-link"></span></a>开始第一个测试案例</h1>

<ul>
<li>启动Spark集群的Mater <code>$SPARK_HOME/sbin/start-master.sh</code>
</li>
<li>启动Spark集群的Slaves <code>$SPARK_HOME/sbin/start-slaves.sh</code>
</li>
<li>启动Hadoop集群<code>/data/hadoop/sbin/start-all.sh</code>
</li>
</ul>

<p><code>hadoop dfs -copyFromLocal ./README.md /</code></p>

<p><code>val file = sc.textFile("hdfs://inside-bigdata04:9000/README.md")</code></p>

<p><code>file.filter(line =&gt; line.contains("Spark"))</code></p>

<h2>
<a id="查看hdfs的文件或文件夹" class="anchor" href="#%E6%9F%A5%E7%9C%8Bhdfs%E7%9A%84%E6%96%87%E4%BB%B6%E6%88%96%E6%96%87%E4%BB%B6%E5%A4%B9" aria-hidden="true"><span class="octicon octicon-link"></span></a>查看HDFS的文件或文件夹</h2>

<ul>
<li><code>hadoop dfs -get  hdfs://10.104.19.122:9000/foo.md</code></li>
<li>查看文件末尾的若干行 <code>hadoop dfs -tail -f hdfs://10.104.19.122:9000/README.md</code>
</li>
<li>列举HDFS的文件或文件夹 <code>hadoop dfs -ls  hdfs://10.104.19.122:9000/</code>
</li>
</ul>

<p>目前推荐如下命令</p>

<p><code>hdfs dfs -ls /</code></p>

<p><code>hdfs dfs -ls  hdfs://$HADOOP_MASTER_ID:9000/</code></p>
      </section>
    </div>

    
  </body>
</html>
