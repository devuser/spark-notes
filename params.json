{"name":"Spark-notes","tagline":"Note anything during writing spark or scala hadoop golfing gopher python review 大数据 面试题 面试 BAT","body":"# sparknotes\r\nNote anything during writing spark or scala hadoop golfing gopher python review 大数据 面试题 面试 BAT\r\n\r\n本系列文档基于Spark 1.5撰写。\r\n\r\n# **机器学习框架（Spark MLlib)**\r\n目前支持4种常见的机器学习问题：二元分类、回归、聚类以及协同过滤 - 依赖 （将会调用jblas线性代数库，这个库本身依赖于原生的Fortran程序 如果想用Python调用MLlib，需要安装NumPy 1.7或更新的版本 - 二元分类\r\n\r\n```\r\n是个监督学习问题。 目前支持两个适用于二元分类的标准模型家族：线性支持向量机（SVMs）和逻辑回归， 同时也分别适用于这两个模型家族的L1和L2正则化变体。\r\n\r\n这些训练算法都利用了一个底层的梯度下降基础算法。 二元分类算法的输入值是一个正则项参数（regParam）和多个与梯度下降相关的参数（stepSize，numIterations，miniBatchFraction）\r\n\r\n目前可用的二元分类算法：\r\n- **SVMWithSGD**\r\n- **LogisticRegressWithSGD**\r\n```\r\n\r\n- 线性回归\r\n- 聚类\r\n- 协同过滤 隐性反馈与协同反馈\r\n- 梯度下降基础算法\r\n- 二元分类\r\n- 线性回归\r\n- 聚类\r\n- 协同过滤\r\n\r\n# 搭建Hadoop单机版本和伪分布式开发环境\r\n`sudo -s`进入**root**用户权限模式\r\n\r\n`apt-get install vim`\r\n\r\nHadoop是采用SSH进行通讯的，此时要设置密码为空， 即不需要密码登录，这样免去每次通讯时都输入密码。\r\n\r\n`apt-get install ssh`\r\n\r\n安装完毕后启动SSH服务\r\n\r\n`/etc/init.d/ssh start`\r\n\r\n以下命令验证服务是否正常启动\r\n\r\n`ps -e|grep ssh`\r\n\r\n如下命令产生私钥和公钥\r\n\r\n`ssh-keygen -t rsa -P \"\"`\r\n\r\n使用Java自带的jps命令查询出所有的守护进程:\r\n\r\n__ @todo插入图片\\__ - 创建HDFS的文件夹_/input_ `bin/hadoop dfs -mkdir /input` - 复制本地的配置文件到HDFS文件夹_/input_ `bin/hadoop dfs -copyFromLocal etc/hadoop/*.xml /input` - 在刚刚构建的伪分布式模式下运行自带的_wordcount_程序 `bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /input /output`\r\n\r\n# 开始第一个测试案例\r\n- 启动Spark集群的Mater `$SPARK_HOME/sbin/start-master.sh`\r\n- 启动Spark集群的Slaves `$SPARK_HOME/sbin/start-slaves.sh`\r\n- 启动Hadoop集群`/data/hadoop/sbin/start-all.sh`\r\n\r\n`hadoop dfs -copyFromLocal ./README.md /`\r\n\r\n`val file = sc.textFile(\"hdfs://inside-bigdata04:9000/README.md\")`\r\n\r\n`file.filter(line => line.contains(\"Spark\"))`\r\n\r\n## 查看HDFS的文件或文件夹\r\n- `hadoop dfs -get  hdfs://10.104.19.122:9000/foo.md`\r\n- 查看文件末尾的若干行 `hadoop dfs -tail -f hdfs://10.104.19.122:9000/README.md`\r\n- 列举HDFS的文件或文件夹 `hadoop dfs -ls  hdfs://10.104.19.122:9000/`\r\n\r\n目前推荐如下命令\r\n\r\n`hdfs dfs -ls /`\r\n\r\n`hdfs dfs -ls  hdfs://$HADOOP_MASTER_ID:9000/`","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}